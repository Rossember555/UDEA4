{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/UDEA-Esp-Analitica-y-Ciencia-de-Datos/EACD-04-MACHINE-LEARNING-1/blob/master/18-[TALLER]_Maquinas_de_Vectores_de_Soporte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "**Recuerda que una vez abierto, Da clic en \"Copiar en Drive\", de lo contrario no podras alamancenar tu progreso**\n",
    "\n",
    "Nota: no olvide ir ejecutando las celdas de código de arriba hacia abajo para que no tenga errores de importación de librerías o por falta de definición de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/UDEA-Esp-Analitica-y-Ciencia-de-Datos/EACD-04-MACHINE-LEARNING-1/master/init.py\n",
    "import init; init.init(force_download=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.lib.general import configure_lab5_2\n",
    "configure_lab5_2()\n",
    "from local.lib.lab5 import *\n",
    "GRADER, dataset = part_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller - Parte 1\n",
    "\n",
    "**Regresión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Limipiar base de datos y completar código\n",
    "\n",
    "En este ejercicio usaremos la regresión por vectores de soporte para resolver el problema de regresión de la base de datos AirQuality [link](https://archive.ics.uci.edu/ml/datasets/Air+Quality). Tener en cuenta que vamos a usar solo 2000 muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta base de datos vamos a realizar una limpieza de datos. Para ello vamos a completar la siguiente función para realizar:\n",
    "1. **Remover** todos registros cuya variable objetivo es faltante (missing Value). Estos registros están marcados como -200, es decir, donde haya un valor -200 eliminaremos el registro.\n",
    "2. **imputar los valores perdidos/faltantes** en cada una de las características, vamos a buscar los valores faltantes y los reemplazaremos por el valor medio de la característica en especifico.\n",
    "3. **Verificar** si quedaron valores faltantes\n",
    "4. ** retornar** X (12 primeras columnas) y Y(la 13 columna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "\n",
    "# Revisar la documentación de: from sklearn.impute import SimpleImputer\n",
    "def clean_data(dataset):\n",
    "    \"\"\"funcion que limpia el dataset y obtiene X y Y\"\"\"\n",
    "    \n",
    "    database = dataset.copy()\n",
    "    \n",
    "    # identificar muetras cuya salida es un valor faltante\n",
    "    idx_to_remove = ...\n",
    "        \n",
    "    #remover las muestras identificadas en el paso anterior\n",
    "    database = ...\n",
    "\n",
    "    print (\"\\nHay \" + str(len(idx_to_remove)-np.sum(idx_to_remove)) + \" muestras con valores perdidos en la variable de salida.\")\n",
    "    print (\"\\nDim de la base de datos removiendo las muestras con variable de salida faltante \"+ str(np.shape(database)))\n",
    "\n",
    "    ##Imputar\n",
    "    print (\"\\nProcesando imputación de valores perdidos en las características . . .\")\n",
    "    imp = SimpleImputer...\n",
    "    \n",
    "    database = ...\n",
    "    \n",
    "    print (\"Imputación finalizada.\\n\")\n",
    "\n",
    "    ##Verificar\n",
    "    missed_values = False\n",
    "    for i in range(0,np.size(database,0)):\n",
    "        if -200 in database[i,:]:\n",
    "            missed_values = True\n",
    "    if(missed_values):\n",
    "        print (\"Hay valores perdidos\")\n",
    "    else:\n",
    "        print (\"No hay valores perdidos en la base de datos. Ahora se puede procesar\")\n",
    "\n",
    "    X = ...\n",
    "    Y = ...\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignorar los prints\n",
    "GRADER.run_test(\"ejercicio1\", clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usemos la función para tener nuestras variables X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = clean_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Experimentar SVM para regresión\n",
    "\n",
    "Ahora vamos a crear la función para experimentar con la regresión por vectores de soporte. Para ello necesitamos:\n",
    "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html. \n",
    "2. Vamos a variar tres parámetros del SVR: kernel,  gamma y el parametro de regularización.\n",
    "3. Utilizar la metodología cross-validation con 5 folds.\n",
    "4. Usar normalización de datos estandar implementada por sklearn\n",
    "5. Extraer el porcentaje de vectores de soporte (observe los *atributos* del modelo SVR de sklearn). Recuerde que estos atributos son accesibles, una vez el modelo es entrenado\n",
    "6. Utilizar el error cuadratico medio de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejercicio de código\n",
    "def experiementarSVR(x, y, kernels, gammas,params_reg):\n",
    "    \"\"\"función que realiza experimentos sobre una SVM para regresión\n",
    "    \n",
    "    x: numpy.Array, con las caracteristicas del problema\n",
    "    y: numpy.Array, con la variable objetivo\n",
    "    kernels: List[str], lista con valores a pasar \n",
    "        a sklearn correspondiente al kernel de la SVM\n",
    "    gammas: List[float], lista con los valores a pasar a\n",
    "        sklean correspondiente el valor de los coeficientes para usar en el\n",
    "        kernel\n",
    "    params_reg: List[float], lista con los valores a a pasar a \n",
    "        sklearn para ser usados como parametro de regularización\n",
    "    \n",
    "    retorna: pd.Dataframe con las siguientes columnas:\n",
    "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
    "        - error cuadratico medio en el cojunto test (promedio de los 5 folds)\n",
    "        - intervalo de confianza del error cuadratico medio en el cojunto test \n",
    "            (desviacion estandar de los 5 folds)\n",
    "        - % de Vectores de Soporte promedio para los 5 folds (0 a 100)\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    kf = ...(n_splits=5)\n",
    "    \n",
    "    \n",
    "    # crear una lista con la combinaciones de los elementos de cada list\n",
    "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
    "    resultados = pd.DataFrame()\n",
    "    \n",
    "    for params in kernels_gammas_regs:\n",
    "        \n",
    "        #parámetros a evaluar\n",
    "        kernel, gamma, param_reg = params \n",
    "        print(\"parametros usados\", params) # puede usar para ver los params\n",
    "        \n",
    "        errores_test = []\n",
    "        pct_support_vectors = []\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            \n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]  \n",
    "            \n",
    "            # normalizar los datos\n",
    "            scaler = ...\n",
    "            X_train = ...\n",
    "            X_test = ...\n",
    "            \n",
    "            \n",
    "            # Entrenar el modelo\n",
    "            svr = ...\n",
    "            \n",
    "            \n",
    "            # Validación del modelo\n",
    "            ypred = ...\n",
    "            \n",
    "            # error y pct de vectores de soporte\n",
    "            errores_test.append(...(y_true = ..., y_pred = ...))\n",
    "            \n",
    "            # Determinar el porcentaje de vectores de soporte\n",
    "            n_train = X_train.shape[0]\n",
    "            pct_vs = (.../ ...)\n",
    "            pct_support_vectors.append(pct_vs)\n",
    "        \n",
    "        resultados.loc[idx,'kernel'] = kernel\n",
    "        resultados.loc[idx,'gamma'] = gamma\n",
    "        resultados.loc[idx,'param_reg'] = param_reg\n",
    "        resultados.loc[idx,'error de validación (promedio)'] = np.mean(errores_test)\n",
    "        resultados.loc[idx,'error de validación (intervalo de confianza)'] = np.std(errores_test)\n",
    "        resultados.loc[idx,'% de vectores de soporte'] = np.mean(pct_support_vectors)\n",
    "        idx+=1\n",
    "    return (resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADER.run_test(\"ejercicio2\", experiementarSVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar vamos a ignorar las dos primeras variables, estas corresponden a valores de fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a realizar los experimentos\n",
    "resultadosSVR = experiementarSVR(x =X_train,y=y_train,\n",
    "                                 kernels=['linear', 'rbf'],\n",
    "                                 gammas = [0.01,0.1],\n",
    "                                 params_reg = [0.1, 1.0,10]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadosSVR.sort_values('error de validación (promedio)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown ¿Cuál es el porcentaje promedio de vectores de soporte para la mejor configuración de hiper-parámetros? \n",
    "respuesta_1 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar los resultados vamos a entrenar nuevamente el modelo con el mejor subconjunto de hiperparámetros, usando todo el conjunto de train y a evaluar en text. Vamos a crear dos graficas para el mejor modelo encontrado:\n",
    "\n",
    "1. Vamos a graficar en el eje x el valor real, en el eje y el valor predicho. El modelo ideal deberia ser una recta que recuerda la identidad\n",
    "2. En el eje x vamos a recorrer las muestras de validación y con colores vamos a diferenciar entre el valor real y el valor predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "# reemplazar por los valores\n",
    "Ypred= predict_svr(X_train,y_train,X_test,kernel = ... , gamma = ..., param_reg = ...)\n",
    "\n",
    "f, ax = plt.subplots(ncols=2, sharex=False, sharey=False, figsize = (22,6))\n",
    "ax[0].scatter(y_test, Ypred)\n",
    "ax[0].set_xlabel('valor real', fontdict = {'fontsize': 12})\n",
    "ax[0].set_ylabel('valor predicho', fontdict = {'fontsize': 12})\n",
    "ax[1].plot(y_test, label = 'valor real', color = 'b', alpha = 0.7)\n",
    "ax[1].plot(Ypred, label = 'valor predicho', color = 'r', alpha = 0.5)\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel('Humedad relativa', fontdict = {'fontsize': 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller - Parte 2\n",
    "\n",
    "**Clasificación**\n",
    "\n",
    "En este ejercicio vamos a volver a resolver el problema de clasificacion de dígitos. Vamos usar solo 4 clases y realizaremos un pre-procesamiento mediante PCA (una técnica de extracción de características que verán en módulos posteriores, dejo un link a un material alternativo sobre el tema [link](https://github.com/jdariasl/ML_2020/blob/master/Clase%2019%20-%20An%C3%A1lisis%20de%20Componentes%20Principales.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcl, Ycl = load_digits(n_class=4,return_X_y=True)\n",
    "#--------- preprocesamiento--------------------\n",
    "pca = PCA(0.99, whiten=True)\n",
    "Xcl = pca.fit_transform(Xcl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ello necesitamos:\n",
    "\n",
    "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "2. Nuevamente vamos a variar tres parámetros del SVC: kernel,  gamma y el parametro de regularización.\n",
    "3. Utilizar una metodología cross-validation con 4 folds que garantice la proporcionalidad original de las clases en los conjuntos de entrenamiento y validación.\n",
    "4. Usar normalización de datos estandar implementada por sklearn\n",
    "5. Calcular el porcentaje de vectores de soporte (observe los *atributos* del modelo SVC de sklearn). Recuerde que estos atributos son accesibles una vez el modelo es entrenado\n",
    "6. Utilizar como error 1 - la exactitud de la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejercicio de código\n",
    "def experiementarSVC(x, y, kernels, gammas,params_reg):\n",
    "    \"\"\"función que realizar experimentos sobre un SVM para clasificación\n",
    "    \n",
    "    x: numpy.Array, con las caracteristicas del problema\n",
    "    y: numpy.Array, con la variable objetivo\n",
    "    kernels: List[str], lista con valores a pasar \n",
    "        a sklearn correspondiente al kernel de la SVM\n",
    "    gammas: List[float], lista con los valores a pasar a\n",
    "        sklean correspondiente el valor de los coeficientes para usar en el\n",
    "        kernel\n",
    "    params_reg: List[float], lista con los valores a a pasar a \n",
    "        sklearn para ser usados como parametro de regularización\n",
    "    \n",
    "    retorna: pd.Dataframe con las siguientes columnas:\n",
    "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
    "        - error de clasificación en entrenamiento (promedio de los 4 folds)\n",
    "        - error de clasificación en validación (promedio de los 4 folds)\n",
    "        - % de Vectores de Soporte promedio para los 4 folds (0 a 100)\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    kf = ...(n_splits=4)\n",
    "    \n",
    "    # crear una lista con la combinaciones de los elementos de cada list\n",
    "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
    "    resultados = pd.DataFrame()\n",
    "    \n",
    "    for params in kernels_gammas_regs:\n",
    "        kernel, gamma, param_reg = params\n",
    "        print(\"parametros usados\", params) # puede usar para ver los params\n",
    "        \n",
    "        errores_train = []\n",
    "        errores_test = []\n",
    "        pct_support_vectors = []\n",
    "        for train_index, test_index in kf.split(x,y):\n",
    "            \n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]  \n",
    "            \n",
    "            # normalizar los datos\n",
    "            scaler = ...\n",
    "            X_train = ...\n",
    "            X_test = ...\n",
    "            \n",
    "            # Entrenar el modelo\n",
    "            svm = ...\n",
    "            \n",
    "            # Realizar predicciones\n",
    "            y_train_pred = ...\n",
    "            y_test_pred = ...\n",
    "            \n",
    "            # Cáluclo de error y pct de vectores de soporte\n",
    "            errores_train.append(...(y_true = ..., y_pred = ...))\n",
    "            errores_test.append(...(y_true = ..., y_pred = ...))\n",
    "            \n",
    "            n_train = X_train.shape[0]\n",
    "            pct_vs = (.../ ... )\n",
    "            pct_support_vectors.append(pct_vs)\n",
    "        \n",
    "        resultados.loc[idx,'kernel'] = kernel\n",
    "        resultados.loc[idx,'gamma'] = gamma\n",
    "        resultados.loc[idx,'param_reg'] = param_reg\n",
    "        resultados.loc[idx,'error de entrenamiento'] = np.mean(errores_train)\n",
    "        resultados.loc[idx,'error de validación'] = np.mean(errores_test)\n",
    "        resultados.loc[idx,'% de vectores de soporte'] = np.mean(pct_support_vectors)\n",
    "        idx+=1\n",
    "    return (resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADER.run_test(\"ejercicio3\", experiementarSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a realizar los experimentos\n",
    "resultadosSVC = experiementarSVC(x = Xcl,y=Ycl,\n",
    "                                 kernels=['linear', 'rbf'],\n",
    "                                 gammas = [0.01,0.1],\n",
    "                                 params_reg = [0.001, 0.01,0.1, 1.0,10]\n",
    "                                )\n",
    "\n",
    "resultadosSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown ¿Cuál es el porcentaje promedio de vectores de soporte para la mejor configuración de hiper-parámetros? \n",
    "respuesta_2 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver la relación de parametro de regularización y los vectores de soporte\n",
    "import seaborn as sns\n",
    "ax= sns.relplot(data = resultadosSVC, x = 'param_reg', y = '% de vectores de soporte', kind = 'line', hue ='kernel', aspect = 1.5)\n",
    "ax.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown ¿Tiene algún efecto el parámetro de regularización en el número de vectores de soporte? \n",
    "respuesta_3 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown Teniendo en cuenta que la SVM es un modelo en principio biclase ¿Qué estrategia usó el método anterior para resolver el problema de clasificación? \n",
    "respuesta_4 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADER.check_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
